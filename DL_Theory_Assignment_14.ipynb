{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwVnJKjbjWt2jWzLZSdr9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KirtiKousik/DL_Theory_Assignments_iNeuron/blob/main/DL_Theory_Assignment_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
      ],
      "metadata": {
        "id": "51kZT5hbp2Dz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No, initializing all the weights to the same value, regardless of the initialization method used, can lead to poor performance in training a neural network. This is because all the neurons in a layer would have the same parameters, leading to the same computations being performed in each neuron. This can lead to symmetry problems, where the gradients flowing through the network during backpropagation will be the same for each neuron, leading to all the neurons learning the same thing. This can lead to a lack of diversity in the representations learned by each neuron, resulting in suboptimal performance.\n",
        "\n",
        "- Therefore, it is recommended to initialize the weights with different random values, such as with the He initialization method, which draws weights from a normal distribution centered around 0, with a standard deviation proportional to the number of inputs to the neuron. This helps to break the symmetry and prevent the gradients from being the same in all neurons, leading to better training performance."
      ],
      "metadata": {
        "id": "YU4mg3Znp_7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Is it okay to initialize the bias terms to 0?"
      ],
      "metadata": {
        "id": "eRBVLKR2qGba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It is generally okay to initialize the bias terms to 0, although this is not always the best choice. Initializing the bias terms to 0 is a simple and reasonable starting point, as it sets the initial activation value of each neuron to zero. This can help ensure that the activations are symmetrical around 0, which can make the optimization problem easier to solve.\n",
        "\n",
        "- However, depending on the nature of the problem and the activation function used, initializing the biases to 0 may not always lead to the best performance. For example, if the activation function is a saturating nonlinearity, such as the sigmoid or tanh functions, then initializing the biases to 0 may cause the activations to be close to the saturation region, leading to poor gradient flow and slow convergence. In such cases, it may be helpful to initialize the biases with small random values to break the symmetry and help ensure good gradient flow.\n",
        "\n",
        "- Ultimately, the optimal choice of bias initialization values depends on the specifics of the problem, the network architecture, and the activation functions used, and may require some experimentation to determine."
      ],
      "metadata": {
        "id": "6eJrf2_bqQ2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Name three advantages of the ELU activation function over ReLU."
      ],
      "metadata": {
        "id": "tRo9Ek_tqKZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i. Avoiding the dying ReLU problem: Unlike the ReLU activation function, the ELU activation function never saturates at 0 for negative inputs. This means that the gradients can flow freely even when the neurons’ activations are very negative, which is useful for preventing the dying ReLU problem.\n",
        "\n",
        "ii. Smoothness: The ELU activation function is a smooth function, unlike the ReLU activation function, which is not differentiable at 0. The smoothness of the ELU activation function can help gradient-based optimization algorithms converge more quickly.\n",
        "\n",
        "iii. Better Mean Activation Value: The mean activation value of ELU neurons is close to 0, which can help prevent the vanishing/exploding gradients problem. This is because the mean activation value for ReLU neurons is around 0.5 for positive inputs and 0 for negative inputs, and this may cause the gradients to vanish or explode."
      ],
      "metadata": {
        "id": "feWcIQ5UqaFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
      ],
      "metadata": {
        "id": "A2b31zKGqVXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The choice of activation function can depend on various factors such as the type of problem you are trying to solve, the distribution of your data, and the architecture of your network. However, here are some general guidelines for when to use each activation function:\n",
        "\n",
        "    1. ELU (Exponential Linear Unit): ELU is a more sophisticated activation function that can help reduce the vanishing gradient problem. It is a good choice for deep networks where ReLU might not perform well.\n",
        "\n",
        "    2. Leaky ReLU: Leaky ReLU is a variant of ReLU that tries to address the dying ReLU problem. It allows a small negative slope for x < 0, which helps prevent the activation from completely dying.\n",
        "\n",
        "    3. ReLU: ReLU is a popular activation function due to its simplicity and efficiency. It is a good choice for shallow networks or as the first layer of a deep network.\n",
        "\n",
        "    4. Tanh: tanh is a hyperbolic tangent function that outputs values between -1 and 1. It is often used in recurrent neural networks or as the activation function in the hidden layers of a feedforward network.\n",
        "\n",
        "    5. Logistic: The logistic function is also known as the sigmoid function. It is often used as the activation function in the output layer of a binary classification problem.\n",
        "\n",
        "    6. Softmax: The softmax activation function is used in the output layer of a multiclass classification problem. It squashes the outputs of each class into a probability distribution, so the sum of all outputs is equal to 1."
      ],
      "metadata": {
        "id": "GLM2DUyQqpRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?"
      ],
      "metadata": {
        "id": "UrCBpoGgqk3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Setting the momentum hyperparameter too close to 1 in a MomentumOptimizer can lead to oscillation and instability in the training process. The momentum optimizer updates the model parameters by adding a fraction of the previous update to the current update. If the momentum is set too close to 1, it may cause the optimizer to make large updates that overshoot the optimal values. This can lead to oscillation and instability in the training process, resulting in slow convergence or even failure to converge.\n",
        "\n",
        "- Additionally, a high momentum value can cause the optimizer to get stuck in a shallow local minimum, rather than reaching the global minimum. To avoid these problems, it is recommended to set the momentum hyperparameter to a value between 0.5 and 0.9, rather than too close to 1."
      ],
      "metadata": {
        "id": "qO2oi9wtqzN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Name three ways you can produce a sparse model.\n"
      ],
      "metadata": {
        "id": "KL9Ajfadq3jA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Three ways to produce a sparse model are:\n",
        "\n",
        "    1. L1 Regularization: This method adds a penalty term proportional to the absolute value of the weights in the loss function. This penalty causes some of the weights to become exactly zero during optimization, resulting in a sparse model.\n",
        "\n",
        "    2. Early Stopping: During the training process, you can monitor the validation loss and stop the training process when it stops improving. This will prevent overfitting, which is known to cause weight explosion, and produce a sparse model.\n",
        "\n",
        "    3. Pruning: This method involves removing some of the weights that have a low magnitude, effectively zeroing them out. Pruning can be applied either during or after the training process. Pruning can be done globally, by removing all the weights below a certain threshold, or locally, by removing the weights with the lowest magnitude in a given layer or neuron."
      ],
      "metadata": {
        "id": "4ZSIWzAJrDJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
      ],
      "metadata": {
        "id": "fwYeo4Ntq5bJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropout can slow down training as it requires extra computation during each iteration to randomly drop neurons. However, it does not slow down inference because during inference, all neurons are used, and no neurons are dropped randomly. The weights of the neurons that were dropped during training are simply scaled by the dropout rate to compensate for the fact that during training, these neurons were only active some of the time."
      ],
      "metadata": {
        "id": "aBw_hhskrIYe"
      }
    }
  ]
}