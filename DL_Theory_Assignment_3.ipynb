{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqbmLDE9yyTLLybnGfEwjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KirtiKousik/DL_Theory_Assignments_iNeuron/blob/main/DL_Theory_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
      ],
      "metadata": {
        "id": "kPV-ZEDzcPpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No, it is not recommended to initialize all the weights to the same value, even if that value is randomly selected using He initialization. This is because the backpropagation algorithm adjusts the weights based on the gradient of the loss function with respect to the weights. If all the weights are initialized to the same value, then the gradients with respect to each weight will be the same, and the weights will be updated in the same way in each iteration of the algorithm. This can result in slow convergence or even getting stuck in a suboptimal solution.\n",
        "\n",
        "- Using He initialization, which is a type of random initialization, is designed to address the issue of initializing the weights in a way that scales with the number of neurons in the layer. The idea behind He initialization is to initialize the weights with a random value that is proportional to the square root of the number of neurons in the layer, so that the weights are in a suitable range for the activation function. This helps to ensure that the activation function is not saturating, which can help to speed up the convergence of the backpropagation algorithm."
      ],
      "metadata": {
        "id": "2VvVgB5sd3Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Is it OK to initialize the bias terms to 0?"
      ],
      "metadata": {
        "id": "lW8IiNwtdpmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Initializing the bias terms to 0 is a common practice, but it may not be optimal for all cases. The bias term is used to shift the activation function to the left or right, and it can play a significant role in the performance of the neural network.\n",
        "\n",
        "- If the activation function is symmetrical, such as the sigmoid or hyperbolic tangent function, initializing the bias terms to 0 may not have a significant impact on the performance. However, in other cases, initializing the bias terms to a small non-zero value, such as 0.01, can help to prevent saturation of the activation function and speed up convergence of the learning algorithm.\n",
        "\n",
        "- Ultimately, the best value to initialize the bias terms to depends on the specific problem being solved, the choice of activation function, and the optimization algorithm being used. In some cases, it may be necessary to experiment with different values for the bias terms to find the best configuration for a particular problem."
      ],
      "metadata": {
        "id": "5rrX6hHmdvSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Name three advantages of the SELU activation function over ReLU."
      ],
      "metadata": {
        "id": "12XnoC03dfIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Scaled Exponential Linear Unit (SELU) activation function has several advantages over the Rectified Linear Unit (ReLU) activation function:\n",
        "\n",
        "    1. Self-Normalizing: The SELU activation function is self-normalizing, which means that it tends to preserve the mean and variance of the activations during the forward pass through the network. This helps to prevent the vanishing or exploding gradient problem that can occur with other activation functions, and can lead to faster convergence and better generalization performance.\n",
        "\n",
        "    2. Non-saturating: Unlike ReLU, the SELU activation function does not saturate at either end, which means that it can continue to learn and update the weights even for large input values. This can help to improve the overall performance of the network.\n",
        "\n",
        "    3. Easier optimization: The SELU activation function has a smooth, non-linear shape that can make it easier to optimize the network weights using gradient-based optimization algorithms such as stochastic gradient descent. The smoothness of the SELU activation function can help to prevent the optimization from getting stuck in suboptimal solutions or local minima.\n",
        "\n",
        "- These advantages make the SELU activation function a popular choice for deep neural networks, especially for those trained using unsupervised or semi-supervised learning methods. However, it is worth noting that the SELU activation function may not be appropriate for all types of problems, and it may be necessary to experiment with different activation functions to find the best one for a particular problem."
      ],
      "metadata": {
        "id": "eZWjYwjFdi4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
      ],
      "metadata": {
        "id": "ygF3uQPbdNgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Each activation function has its own strengths and weaknesses, and the choice of activation function depends on the specific requirements of the problem being solved. Here is a general guide for when to use each activation function:\n",
        "\n",
        "    1. SELU (Scaled Exponential Linear Unit): SELU activation is appropriate for deep neural networks, especially for those trained using unsupervised or semi-supervised learning methods. It helps to prevent the vanishing or exploding gradient problem that can occur with other activation functions and has a smooth, non-linear shape that can make it easier to optimize the network weights.\n",
        "\n",
        "    2. Leaky ReLU (and its variants): Leaky ReLU is a variant of ReLU that allows for a small, non-zero gradient when the input is negative. This helps to address the problem of \"dying ReLUs\" where the activation function becomes stuck at 0 and can no longer learn. Leaky ReLU can be a good choice for large, sparsely-connected networks, where it may be difficult for the activations to propagate through the network.\n",
        "\n",
        "    3. ReLU (Rectified Linear Unit): ReLU is a popular activation function for feedforward networks and is particularly well-suited for problems involving sparse data, where many input features are 0 or near 0. ReLU is computationally efficient and can lead to faster convergence than other activation functions. However, it can be prone to the \"dying ReLU\" problem, where neurons can become stuck and unable to learn.\n",
        "\n",
        "    4. tanh (Hyperbolic Tangent): tanh is a common activation function for recurrent neural networks, where it is used to model the range of real numbers. It has a smooth, sigmoid-like shape that makes it easy to optimize and can help to prevent the optimization from getting stuck in suboptimal solutions or local minima.\n",
        "\n",
        "    5. Logistic: The logistic activation function is similar in shape to the sigmoid function and can be used for binary classification problems. It has a smooth, sigmoidal shape that makes it easy to optimize, but it can be prone to the vanishing gradient problem, especially in deep networks.\n",
        "\n",
        "    6. Softmax: Softmax is an activation function that is used in the output layer of a neural network to produce a probability distribution over multiple classes. It is commonly used for multiclass classification problems, where the goal is to predict the class with the highest probability.\n",
        "\n",
        "- It is worth noting that these activation functions are not mutually exclusive, and it is common to use multiple activation functions in a single network, depending on the requirements of the problem and the architecture of the network."
      ],
      "metadata": {
        "id": "rJUIs50edSuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
      ],
      "metadata": {
        "id": "4CQfd-bfdHHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If the momentum hyperparameter is set too close to 1, it can lead to overshooting and oscillation in the optimization process. This is because a high momentum value puts a heavy weight on the previous update, leading to large updates that may go far away from the optimal solution. These large updates may lead to overshooting the optimal solution and then oscillating back and forth, making it difficult for the optimizer to converge to the optimal solution."
      ],
      "metadata": {
        "id": "fzYsvYBVdKiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Name three ways you can produce a sparse model."
      ],
      "metadata": {
        "id": "e0XV8Arsc4pE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Three ways to produce a sparse model are:\n",
        "\n",
        "    1. Regularization techniques: Regularization techniques such as L1 or Lasso regularization can be applied to the loss function to encourage sparse solutions.\n",
        "\n",
        "    2. Dropout: Dropout is a technique where neurons are randomly dropped during training, forcing the network to learn multiple redundant representations and reducing the risk of overfitting.\n",
        "\n",
        "    3. Pruning: Pruning is a technique where the least important connections are removed from the network, reducing its size and making it more computationally efficient.\n",
        "\n",
        "- Each of these methods helps reduce the number of connections or neurons in the network, leading to a sparse model with fewer parameters."
      ],
      "metadata": {
        "id": "z4nHfW6oc90_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
      ],
      "metadata": {
        "id": "1mOxXomocvO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropout can slow down the training process as it reduces the number of active neurons in the network and hence reduces the learning capacity. However, the performance improvement obtained from using Dropout in terms of reduced overfitting compensates for the slower training.\n",
        "\n",
        "- Dropout does not slow down inference as it is only applied during training, not during evaluation or prediction.\n",
        "\n",
        "- MC Dropout is a variant of Dropout where multiple predictions are made with dropout applied on different neurons and an average of the predictions is taken. MC Dropout adds some computational overhead during inference but provides an estimate of the model's uncertainty, which can be useful in certain applications."
      ],
      "metadata": {
        "id": "i-373OYsczo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
        "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
        "point of this exercise). Use He initialization and the ELU activation function."
      ],
      "metadata": {
        "id": "JVL5QTfrVvW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.initializers import he_normal\n",
        "\n",
        "# load CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# flatten the data\n",
        "x_train = np.reshape(x_train, (len(x_train), -1))\n",
        "x_test = np.reshape(x_test, (len(x_test), -1))\n",
        "\n",
        "# convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# build the model\n",
        "model = Sequential()\n",
        "\n",
        "# add hidden layers\n",
        "for i in range(20):\n",
        "    model.add(Dense(100, input_shape=(3072,), kernel_initializer=he_normal()))\n",
        "    model.add(Activation('elu'))\n",
        "\n",
        "# add output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# evaluate the model\n",
        "_, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ5UKoEiWC8C",
        "outputId": "121aee12-f565-401a-d1f4-d0d3f420d261"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 25s 15ms/step - loss: 2.0760 - accuracy: 0.2308 - val_loss: 1.9725 - val_accuracy: 0.2841\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9007 - accuracy: 0.3009 - val_loss: 1.8252 - val_accuracy: 0.3276\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 1.8289 - accuracy: 0.3328 - val_loss: 1.8153 - val_accuracy: 0.3504\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7880 - accuracy: 0.3490 - val_loss: 1.7478 - val_accuracy: 0.3803\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 1.7458 - accuracy: 0.3679 - val_loss: 1.7035 - val_accuracy: 0.3917\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 1.8840 - accuracy: 0.3054 - val_loss: 1.7664 - val_accuracy: 0.3545\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.7715 - accuracy: 0.3585 - val_loss: 1.7220 - val_accuracy: 0.3605\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7151 - accuracy: 0.3834 - val_loss: 1.7427 - val_accuracy: 0.3810\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 1.6952 - accuracy: 0.3922 - val_loss: 2.3774 - val_accuracy: 0.1797\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 22s 14ms/step - loss: 2.3859 - accuracy: 0.2575 - val_loss: 2.2664 - val_accuracy: 0.1687\n",
            "Test accuracy: 0.16869999468326569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
        "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
        "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
        "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
        "Remember to search for the right learning rate each time you change the model’s\n",
        "architecture or hyperparameters."
      ],
      "metadata": {
        "id": "JRR6dFXrWmZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.optimizers import Nadam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data by scaling the input features\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Convert the class labels to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the DNN model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
        "\n",
        "for i in range(20):\n",
        "    model.add(Dense(100, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model with Nadam optimization and categorical cross-entropy loss\n",
        "optimizer = Nadam(lr=0.002)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stopping = EarlyStopping(patience=10)\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMf-mQyCWeKg",
        "outputId": "ca3f60b9-15b6-4800-fe25-91e3b5a959f6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 60s 31ms/step - loss: 2.4032 - accuracy: 0.1001 - val_loss: 2.3015 - val_accuracy: 0.0972\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 2.2416 - accuracy: 0.1326 - val_loss: 2.1980 - val_accuracy: 0.1625\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 2.1229 - accuracy: 0.1658 - val_loss: 2.0364 - val_accuracy: 0.1825\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.0822 - accuracy: 0.1733 - val_loss: 2.0308 - val_accuracy: 0.1920\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.0629 - accuracy: 0.1759 - val_loss: 2.0730 - val_accuracy: 0.1934\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.0514 - accuracy: 0.1851 - val_loss: 1.9737 - val_accuracy: 0.1997\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.0432 - accuracy: 0.1854 - val_loss: 1.9750 - val_accuracy: 0.1988\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 2.0347 - accuracy: 0.1929 - val_loss: 2.1011 - val_accuracy: 0.1846\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 2.0225 - accuracy: 0.2001 - val_loss: 1.9518 - val_accuracy: 0.2316\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.0158 - accuracy: 0.2075 - val_loss: 1.9338 - val_accuracy: 0.2329\n",
            "Test loss: 1.9337533712387085\n",
            "Test accuracy: 0.2328999936580658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
        "converging faster than before? Does it produce a better model? How does it affect\n",
        "training speed?"
      ],
      "metadata": {
        "id": "FbJT88XIW1rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.optimizers import Nadam\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocessing data\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Building the model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
        "\n",
        "for _ in range(20):\n",
        "    model.add(Dense(100, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "optimizer = Nadam(lr=0.002)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.1, verbose=2,\n",
        "                    callbacks=[keras.callbacks.EarlyStopping(patience=5)])\n",
        "\n",
        "# Evaluating the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U_oMXKXW35-",
        "outputId": "ec1b7a2d-5b1c-4012-a7e1-5c96be54c285"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1407/1407 - 41s - loss: 1.8574 - accuracy: 0.3318 - val_loss: 2.0722 - val_accuracy: 0.2824 - 41s/epoch - 29ms/step\n",
            "Epoch 2/10\n",
            "1407/1407 - 29s - loss: 1.7076 - accuracy: 0.3890 - val_loss: 2.8759 - val_accuracy: 0.2300 - 29s/epoch - 21ms/step\n",
            "Epoch 3/10\n",
            "1407/1407 - 30s - loss: 1.6429 - accuracy: 0.4144 - val_loss: 1.9854 - val_accuracy: 0.3478 - 30s/epoch - 21ms/step\n",
            "Epoch 4/10\n",
            "1407/1407 - 34s - loss: 1.5914 - accuracy: 0.4334 - val_loss: 1.9337 - val_accuracy: 0.3558 - 34s/epoch - 24ms/step\n",
            "Epoch 5/10\n",
            "1407/1407 - 28s - loss: 1.5392 - accuracy: 0.4520 - val_loss: 1.7739 - val_accuracy: 0.3968 - 28s/epoch - 20ms/step\n",
            "Epoch 6/10\n",
            "1407/1407 - 29s - loss: 1.4988 - accuracy: 0.4689 - val_loss: 1.6527 - val_accuracy: 0.4152 - 29s/epoch - 20ms/step\n",
            "Epoch 7/10\n",
            "1407/1407 - 29s - loss: 1.4582 - accuracy: 0.4804 - val_loss: 1.6069 - val_accuracy: 0.4254 - 29s/epoch - 20ms/step\n",
            "Epoch 8/10\n",
            "1407/1407 - 29s - loss: 1.4247 - accuracy: 0.4957 - val_loss: 1.8765 - val_accuracy: 0.3444 - 29s/epoch - 20ms/step\n",
            "Epoch 9/10\n",
            "1407/1407 - 28s - loss: 1.3941 - accuracy: 0.5068 - val_loss: 1.6309 - val_accuracy: 0.4214 - 28s/epoch - 20ms/step\n",
            "Epoch 10/10\n",
            "1407/1407 - 28s - loss: 1.3768 - accuracy: 0.5134 - val_loss: 1.4797 - val_accuracy: 0.4668 - 28s/epoch - 20ms/step\n",
            "Test Loss: 1.4912925958633423\n",
            "Test Accuracy: 0.46560001373291016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes, it is converging faster than before.\n",
        "- Yes, it produces a better model.\n",
        "- It increases the training speed."
      ],
      "metadata": {
        "id": "YFAqArMaXA8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
        "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
        "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
        "layers, etc.)."
      ],
      "metadata": {
        "id": "y0jyZy5iXdvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Standardizing the input features\n",
        "mean = np.mean(x_train, axis=(0,1,2))\n",
        "std = np.std(x_train, axis=(0,1,2))\n",
        "x_train = (x_train - mean) / std\n",
        "x_test = (x_test - mean) / std\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
        "\n",
        "# Add 20 hidden layers with 100 neurons each\n",
        "for i in range(20):\n",
        "    model.add(Dense(100, activation='selu',\n",
        "                    kernel_initializer='lecun_normal'))\n",
        "\n",
        "# Softmax output layer with 10 neurons\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Nadam(lr=0.002),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    verbose=2,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[EarlyStopping(patience=5)])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHrN2f2-XYiA",
        "outputId": "b5101bda-e937-4add-cf48-dff4a63ad1ad"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 - 26s - loss: 2.0841 - accuracy: 0.2184 - val_loss: 2.0861 - val_accuracy: 0.1912 - 26s/epoch - 17ms/step\n",
            "Epoch 2/10\n",
            "1563/1563 - 23s - loss: 2.0056 - accuracy: 0.2292 - val_loss: 1.9787 - val_accuracy: 0.2517 - 23s/epoch - 15ms/step\n",
            "Epoch 3/10\n",
            "1563/1563 - 21s - loss: 1.9755 - accuracy: 0.2422 - val_loss: 1.9767 - val_accuracy: 0.2418 - 21s/epoch - 14ms/step\n",
            "Epoch 4/10\n",
            "1563/1563 - 22s - loss: 2.2852 - accuracy: 0.1318 - val_loss: 2.1844 - val_accuracy: 0.1832 - 22s/epoch - 14ms/step\n",
            "Epoch 5/10\n",
            "1563/1563 - 23s - loss: 2.0966 - accuracy: 0.2123 - val_loss: 2.0942 - val_accuracy: 0.1856 - 23s/epoch - 14ms/step\n",
            "Epoch 6/10\n",
            "1563/1563 - 24s - loss: 2.0205 - accuracy: 0.2076 - val_loss: 1.9693 - val_accuracy: 0.2378 - 24s/epoch - 15ms/step\n",
            "Epoch 7/10\n",
            "1563/1563 - 21s - loss: 1.9916 - accuracy: 0.2176 - val_loss: 1.9587 - val_accuracy: 0.2519 - 21s/epoch - 14ms/step\n",
            "Epoch 8/10\n",
            "1563/1563 - 27s - loss: 1.9516 - accuracy: 0.2432 - val_loss: 2.0056 - val_accuracy: 0.2240 - 27s/epoch - 17ms/step\n",
            "Epoch 9/10\n",
            "1563/1563 - 23s - loss: 1.8878 - accuracy: 0.2688 - val_loss: 1.9386 - val_accuracy: 0.2643 - 23s/epoch - 15ms/step\n",
            "Epoch 10/10\n",
            "1563/1563 - 21s - loss: 1.9190 - accuracy: 0.2588 - val_loss: 1.8959 - val_accuracy: 0.2729 - 21s/epoch - 13ms/step\n",
            "Test accuracy: 0.2728999853134155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
        "see if you can achieve better accuracy using MC Dropout."
      ],
      "metadata": {
        "id": "DemONxNHXxSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Nadam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Build the deep neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(100, activation='elu', input_shape=(32 * 32 * 3,)))\n",
        "for _ in range(19):\n",
        "    model.add(Dense(100, activation='elu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Nadam(lr=0.002), metrics=['accuracy'])\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stopping = EarlyStopping(patience=5)\n",
        "model.fit(x_train.reshape(-1, 32 * 32 * 3), y_train, batch_size=32, epochs=10, validation_data=(x_test.reshape(-1, 32 * 32 * 3), y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Regularize the model with alpha dropout\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Evaluate the model with MC Dropout\n",
        "y_probas = np.stack([model.predict(x_test.reshape(-1, 32 * 32 * 3)) for sample in range(100)])\n",
        "y_pred = y_probas.mean(axis=0)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print('MC Dropout Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJV8t0H0Xys_",
        "outputId": "bc649473-208f-4a47-feed-cdc92cd538bb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 38s 21ms/step - loss: 2.2408 - accuracy: 0.1400 - val_loss: 2.1073 - val_accuracy: 0.1695\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 2.0866 - accuracy: 0.1847 - val_loss: 2.0900 - val_accuracy: 0.2018\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 2.0343 - accuracy: 0.2091 - val_loss: 2.0860 - val_accuracy: 0.2168\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0382 - accuracy: 0.2141 - val_loss: 1.9884 - val_accuracy: 0.2353\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 27s 18ms/step - loss: 2.1710 - accuracy: 0.1601 - val_loss: 2.0673 - val_accuracy: 0.1801\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 1.9971 - accuracy: 0.2254 - val_loss: 1.9506 - val_accuracy: 0.2372\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.9868 - accuracy: 0.2280 - val_loss: 2.0466 - val_accuracy: 0.1822\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.9953 - accuracy: 0.2259 - val_loss: 2.0459 - val_accuracy: 0.2054\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 2.1958 - accuracy: 0.2113 - val_loss: 2.0284 - val_accuracy: 0.2104\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 2.0112 - accuracy: 0.2199 - val_loss: 1.9879 - val_accuracy: 0.2190\n",
            "313/313 [==============================] - 3s 8ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 3s 9ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 5ms/step\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "313/313 [==============================] - 3s 9ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 5ms/step\n",
            "313/313 [==============================] - 1s 5ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 3s 9ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 5ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 1s 4ms/step\n",
            "313/313 [==============================] - 2s 6ms/step\n",
            "MC Dropout Accuracy: 0.219\n"
          ]
        }
      ]
    }
  ]
}