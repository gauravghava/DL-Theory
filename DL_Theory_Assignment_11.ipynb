{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN0sRXJ4zAE5rUK2I8iWgZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KirtiKousik/DL_Theory_Assignments_iNeuron/blob/main/DL_Theory_Assignment_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Write the Python code to implement a single neuron."
      ],
      "metadata": {
        "id": "IQgwdLWBe6__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "        return weighted_sum\n",
        "\n",
        "inputs = np.array([1, 2, 3])\n",
        "weights = np.array([0.1, 0.2, 0.3])\n",
        "bias = 0.1\n",
        "neuron = Neuron(weights, bias)\n",
        "\n",
        "output = neuron.forward(inputs)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MK7-44ce73a",
        "outputId": "b74b78e7-05c8-47a8-e789-28f82b1e5cd1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Write the Python code to implement ReLU."
      ],
      "metadata": {
        "id": "yRAQLDX6fOJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return max(0, x)\n",
        "# This function takes a single input x and returns 0 if x is less than 0, and x otherwise."
      ],
      "metadata": {
        "id": "wkOdpwKDfZ07"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Write the Python code for a dense layer in terms of matrix multiplication."
      ],
      "metadata": {
        "id": "8f2FThlofSu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
        "        self.weights = np.random.randn(input_dim, output_dim)\n",
        "        self.bias = np.zeros((1, output_dim)) if use_bias else None\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Performs matrix multiplication and returns the output of the dense layer.\"\"\"\n",
        "        output = np.dot(inputs, self.weights)\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        return output"
      ],
      "metadata": {
        "id": "8nDa8cZ7fHXg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python)."
      ],
      "metadata": {
        "id": "9GMr7zm9lE1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_layer(inputs, weights, biases):\n",
        "    outputs = [sum(input_ * weight_) + bias_ for input_, weight_ in zip(inputs, weights) for bias_ in biases]\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "uaZ5QkLekmRP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What is the “hidden size” of a layer?"
      ],
      "metadata": {
        "id": "pKzkztQmljuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The \"hidden size\" of a layer refers to the number of neurons or nodes in that layer. It represents the number of internal representation or features learned by the network in that layer. The hidden size determines the capacity or expressiveness of the network and can affect its ability to model the relationships in the data. Increasing the hidden size increases the capacity of the network but also the risk of overfitting, whereas decreasing the hidden size reduces the capacity and the risk of overfitting. The choice of the hidden size is often a trade-off between modeling power and overfitting risk."
      ],
      "metadata": {
        "id": "egAZlmv0luLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. What does the t method do in PyTorch?"
      ],
      "metadata": {
        "id": "o68xRVCWlr07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In PyTorch, the t method is used to transpose a tensor. It transposes the dimensions of the tensor and returns a new tensor with the same data as the original tensor but with the dimensions rearranged according to the specified transpose order.\n",
        "\n",
        "- For example, if a is a 2D tensor with shape (m, n), then a.t() returns a tensor with shape (n, m). The elements of the transposed tensor correspond to the elements of the original tensor but with the rows and columns exchanged."
      ],
      "metadata": {
        "id": "8Gd5ofbIl4dU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Why is matrix multiplication written in plain Python very slow?"
      ],
      "metadata": {
        "id": "Vxjby1UGly63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Matrix multiplication written in plain Python can be slow due to its implementation being in a high-level interpreted language, which can result in a lack of performance compared to low-level, compiled languages like C++. Additionally, the plain Python code is not optimized for matrix multiplication, as it may not take advantage of hardware acceleration such as GPU or TPU processing, or take advantage of parallel processing and vectorization, which are common optimization techniques used in numerical computing libraries such as NumPy, TensorFlow, and PyTorch.\n",
        "\n",
        "- Therefore, to perform matrix multiplication efficiently in Python, it is usually recommended to use a numerical computing library like NumPy, TensorFlow, or PyTorch, which are optimized for matrix multiplication and other numerical operations."
      ],
      "metadata": {
        "id": "xAtUeHcwmBa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. In matmul, why is ac==br?"
      ],
      "metadata": {
        "id": "Y8pySb3bl8l_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In matrix multiplication, the number of columns in matrix A must be equal to the number of rows in matrix B, so ac must be equal to br. This condition is necessary because the matrix product AB is defined only if the number of columns in A is equal to the number of rows in B. If ac is not equal to br, it will result in an error or incorrect results."
      ],
      "metadata": {
        "id": "YcKatZVNmI0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?"
      ],
      "metadata": {
        "id": "ei3nTvdbmEmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can measure the time taken for a single cell to execute in Jupyter Notebook by using the %timeit magic command. This command runs the code in the cell multiple times and returns the best run time.\n",
        "\n",
        "- Here's an example:"
      ],
      "metadata": {
        "id": "2a7Vaf_CmRYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit\n",
        "# code to be executed here"
      ],
      "metadata": {
        "id": "qqt-wDdimUuS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Alternatively, we can use the time module in Python to measure the time taken for a cell to execute:"
      ],
      "metadata": {
        "id": "Q2SjY_YDmc0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# code to be executed here\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de70AwF5mafu",
        "outputId": "d2e23b09-1b5d-4056-86ff-c07a11ebafe1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 4.482269287109375e-05 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. What is elementwise arithmetic?"
      ],
      "metadata": {
        "id": "DEcOpiIdmL5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Elementwise arithmetic refers to mathematical operations performed between two tensors (multi-dimensional arrays) of the same shape on an element-by-element basis. This means that the operation is applied individually to each element of the tensors, without considering their relationships with other elements. For example, if two tensors A and B have the same shape, and we perform the elementwise addition operation A + B, then the resulting tensor C will have the same shape as A and B, with each element c_ij = a_ij + b_ij. The most common elementwise arithmetic operations include addition, subtraction, multiplication, and division."
      ],
      "metadata": {
        "id": "QWuj-BT0mq_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Write the PyTorch code to test whether every element of a is greater than the corresponding element of b."
      ],
      "metadata": {
        "id": "2ddKouHCmtLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([1, 2, 3, 4, 5])\n",
        "b = torch.tensor([5, 4, 3, 2, 1])\n",
        "\n",
        "result = (a > b).all().item()\n",
        "\n",
        "if result == 1:\n",
        "    print(\"All elements of a are greater than the corresponding elements of b.\")\n",
        "else:\n",
        "    print(\"Not all elements of a are greater than the corresponding elements of b.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VaselDUm7jw",
        "outputId": "2b8b4cd3-b8b5-4828-8a4d-6b9828f49491"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not all elements of a are greater than the corresponding elements of b.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. What is a rank-0 tensor? How do you convert it to a plain Python data type?"
      ],
      "metadata": {
        "id": "OfuMpCBMmxb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A rank-0 tensor, also known as a scalar tensor, is a tensor with only one value. It has no shape or dimension. In PyTorch, it is represented by a tensor with shape (). To convert a rank-0 tensor to a plain Python data type (such as an int or a float), we can use the item method. For example:"
      ],
      "metadata": {
        "id": "eK3K_vF1nBxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tensor = torch.tensor(10.0)\n",
        "value = tensor.item()\n",
        "print(value) # prints 10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIsPLDb8nFYH",
        "outputId": "f40b68d8-689f-4034-9733-0a67c7a81d90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If the tensor contains multiple values, we will need to convert it to a plain Python array or matrix first, and then extract the desired value."
      ],
      "metadata": {
        "id": "81LX5_-Tnap5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. How does elementwise arithmetic help us speed up matmul?"
      ],
      "metadata": {
        "id": "SPPw6EdnmzxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Elementwise arithmetic is a fundamental operation in matrix multiplication, as it allows for efficient implementation of many matrix operations. In matrix multiplication, elementwise arithmetic is used to perform a single operation on each element of the matrices involved in the operation. This is much faster than performing a single operation on the entire matrix at once, as the number of operations that need to be performed is reduced.\n",
        "\n",
        "- Elementwise arithmetic can be used to speed up matrix multiplication by breaking the operation into smaller operations on individual elements. For example, the dot product of two matrices can be computed by performing an elementwise multiplication followed by a sum of the results. By parallelizing the elementwise operations, the matrix multiplication can be computed much faster, as the time taken is proportional to the number of elements in the matrices.\n",
        "\n",
        "- In PyTorch, elementwise arithmetic is supported through the use of broadcast semantics, which automatically broadcast scalars and tensors to the correct size for elementwise operations. This makes it easy to write code for matrix multiplication that is both efficient and readable."
      ],
      "metadata": {
        "id": "SG1dVl2QnqI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. What are the broadcasting rules?"
      ],
      "metadata": {
        "id": "qXtW6eMbng1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In PyTorch (and in most numerical libraries), broadcasting is a mechanism that allows operations to be performed on arrays of different shapes, as long as some rules are followed. The rules for broadcasting are as follows:\n",
        "\n",
        "    - If two arrays have different numbers of dimensions, the array with fewer dimensions is padded with 1s on the left.\n",
        "\n",
        "    - If the shape of two arrays does not match on a particular dimension, the size of the smaller array in that dimension is expanded to match the size of the larger array.\n",
        "\n",
        "    - If either of the arrays has a size of 1 in a particular dimension, the array is stretched to match the size of the other array in that dimension.\n",
        "\n",
        "- In general, broadcasting can be used to perform elementwise operations between arrays of different shapes, as long as their shapes are broadcastable to a common shape. If the shapes are not broadcastable, a runtime error will be raised."
      ],
      "metadata": {
        "id": "OLT8oyBknxrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. What is expand_as? Show an example of how it can be used to match the results of broadcasting."
      ],
      "metadata": {
        "id": "RGjRtR0unjR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In PyTorch, \"expand_as\" is a function that takes a tensor \"t\" and a tensor \"other\" as inputs, and expands the size of \"t\" to match that of \"other\". The tensor \"t\" will be expanded with size \"1\" along the dimensions that are missing in \"t\", but present in \"other\".\n",
        "\n",
        "- Here is an example of how \"expand_as\" can be used to match the results of broadcasting:"
      ],
      "metadata": {
        "id": "Lp3Pvzqon9c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define tensor t with shape (3,)\n",
        "t = torch.tensor([1, 2, 3])\n",
        "\n",
        "# Define tensor other with shape (3, 3)\n",
        "other = torch.tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\n",
        "\n",
        "# Expand t to match the shape of other\n",
        "t_expanded = t.unsqueeze(1).expand_as(other)\n",
        "\n",
        "# The shape of t_expanded is now (3, 3)\n",
        "print(t_expanded.shape)\n",
        "\n",
        "# The expanded tensor can now be used in operations with other\n",
        "result = t_expanded + other\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BffYduRyoRMH",
        "outputId": "1d5a8996-2c7d-4313-a523-be0df248f29e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3])\n",
            "tensor([[11, 21, 31],\n",
            "        [42, 52, 62],\n",
            "        [73, 83, 93]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this example, the shape of \"t\" is expanded from \"(3,)\" to \"(3, 3)\" to match the shape of \"other\" so that they can be used in element-wise operations such as addition."
      ],
      "metadata": {
        "id": "2aa0VITUn9YC"
      }
    }
  ]
}