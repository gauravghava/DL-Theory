{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbuWNL10rWrkqyRNjiU8be",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KirtiKousik/DL_Theory_Assignments_iNeuron/blob/main/DL_Theory_Assignment_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Explain the Activation Functions in your own language\n",
        "    a) sigmoid\n",
        "    b) tanh\n",
        "    c) ReLU\n",
        "    d) ELU\n",
        "    e) LeakyReLU\n",
        "    f) swish"
      ],
      "metadata": {
        "id": "fsOUgjbWuMze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Activation functions are mathematical functions that are used to determine the output of a neuron in an artificial neural network. They help the network make predictions based on input data and can help prevent overfitting. Here is an explanation of each activation function:\n",
        "\n",
        "    1. Sigmoid: The sigmoid function is an S-shaped curve that outputs values between 0 and 1. It is used in the output layer when the desired output is binary, for example, in binary classification problems.\n",
        "\n",
        "    2. Tanh: The tanh function is similar to the sigmoid function, but its output values range from -1 to 1. It is commonly used in the hidden layers of a network to introduce nonlinearity.\n",
        "\n",
        "    3. ReLU: The ReLU activation function outputs the input value if it is positive, and outputs 0 if the input value is negative. This function is computationally efficient and has become a popular choice for hidden layers in deep networks.\n",
        "\n",
        "    4. ELU: The ELU activation function is similar to ReLU but includes a negative slope for negative inputs. This helps prevent the vanishing gradient problem, which can occur in deep networks using ReLU.\n",
        "\n",
        "    5. Leaky ReLU: The Leaky ReLU activation function is similar to ReLU, but includes a small positive slope for negative inputs. This helps prevent the activation function from completely dying in cases where the input is negative.\n",
        "\n",
        "    6. Swish: The Swish activation function is a new addition to the list of activation functions. It is similar to the sigmoid function, but has the advantage of being trainable and thus adaptable to the data. It is showing promising results in some deep learning applications and is becoming a popular alternative to ReLU and its variants."
      ],
      "metadata": {
        "id": "KPKIhFOFui3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What happens when you increase or decrease the optimizer learning rate?"
      ],
      "metadata": {
        "id": "BtyT-VOQuskx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The learning rate is a hyperparameter that controls how much the model parameters are updated with each iteration of training. Increasing the learning rate will result in larger updates to the model parameters, allowing the model to learn faster. However, if the learning rate is set too high, the model may oscillate or diverge, rather than converge to a good solution. On the other hand, decreasing the learning rate will result in smaller updates to the model parameters, allowing the model to learn more slowly. However, if the learning rate is set too low, the model may converge too slowly or get stuck in a local minimum.\n",
        "\n",
        "- In practice, the learning rate is typically set using a process called hyperparameter tuning, where multiple values of the learning rate are tried and the one that results in the best performance is selected. The learning rate may also be scheduled or adapted dynamically during training, such as by reducing the learning rate as the training progresses to allow the model to converge more effectively."
      ],
      "metadata": {
        "id": "Bn9I7HXku12n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. What happens when you increase the number of internal hidden neurons?"
      ],
      "metadata": {
        "id": "y_wfU4u8uxAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Increasing the number of internal hidden neurons (also known as the width of the neural network) can lead to an increase in the capacity of the model, allowing it to fit more complex relationships between the input and output. This can lead to better performance on the training data and possibly even on the test data.\n",
        "\n",
        "- However, it can also lead to overfitting, where the model becomes too complex and begins to memorize the training data, rather than generalizing well to new, unseen data. Additionally, increasing the number of neurons can also increase the time it takes to train the model and the memory required to store the parameters.\n",
        "\n",
        "- Therefore, the number of neurons in the hidden layer(s) must be selected carefully, using techniques such as cross-validation to determine the optimal number."
      ],
      "metadata": {
        "id": "BT6xhFA6u84F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What happens when you increase the size of batch computation?"
      ],
      "metadata": {
        "id": "mbOeJOThu4or"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Increasing the size of batch computation, also known as the batch size, can have a significant impact on the performance of a deep learning model.\n",
        "\n",
        "- Generally speaking, increasing the batch size can speed up training, as larger batches can take advantage of parallel computation on modern GPUs. This can result in faster convergence and better performance. However, there are some trade-offs to consider.\n",
        "\n",
        "- If the batch size is too small, the training will be slow, as the model will have to make many weight updates for each epoch, and the noise in the gradients may cause the model to oscillate and slow down convergence. On the other hand, if the batch size is too large, the model may not fit the training data well as the gradients will be less representative of the true underlying data distribution.\n",
        "\n",
        "- Additionally, larger batch sizes require more memory, which can cause issues on systems with limited resources. For example, if the batch size is too large, it may not be possible to fit the batch into memory, and the training process will be slowed down as the model is forced to load the data in smaller chunks.\n",
        "\n",
        "- In summary, there is a trade-off between batch size and training performance. The ideal batch size will depend on the specific deep learning architecture, the size of the dataset, and the computational resources available."
      ],
      "metadata": {
        "id": "hm2mhFlPvHuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Why we adopt regularization to avoid overfitting?"
      ],
      "metadata": {
        "id": "4Jl2SaahvAKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Overfitting occurs when a model is trained too well on the training data, capturing not only the underlying patterns in the data, but also the noise or random fluctuations. This causes the model to perform poorly on new, unseen data. Regularization is used to avoid overfitting by adding a penalty term to the loss function that the model optimizes during training. This penalty term discourages the model from assigning too much importance to any one feature and forces the model to learn more general patterns in the data. Common regularization techniques include L1 and L2 regularization, dropout, and early stopping. By using these techniques, we can prevent overfitting and produce models that are more robust and perform well on new, unseen data."
      ],
      "metadata": {
        "id": "MI-TPQGBvPX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. What are loss and cost functions in deep learning?"
      ],
      "metadata": {
        "id": "8GTpGe4YvCEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In deep learning, the loss function and the cost function are used to evaluate the performance of a model. The loss function measures the difference between the model's predictions and the true labels, while the cost function aggregates the loss over many examples to give an overall measure of the model's performance.\n",
        "\n",
        "- The loss function is an essential component of the training process. It provides feedback to the model about how well it is doing, and helps to guide the optimization algorithm towards a better solution. The goal of the optimization process is to find the set of weights that minimizes the loss function.\n",
        "\n",
        "- The cost function is simply a mathematical expression that represents the average loss over a set of examples. It is used to monitor the progress of the training process and to compare different models. The cost function is typically plotted over time to produce a learning curve that can be used to diagnose problems and to select the best model.\n",
        "\n",
        "- Some commonly used loss functions include mean squared error, categorical cross-entropy, and binary cross-entropy. The choice of loss function depends on the type of problem being solved and the nature of the data."
      ],
      "metadata": {
        "id": "JX8rnnmFvn4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. What do ou mean by underfitting in neural networks?"
      ],
      "metadata": {
        "id": "6rVMIS_rvRm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Underfitting in neural networks occurs when the model is too simple and unable to capture the underlying patterns and relationships in the data. This results in a model that has high bias, meaning that it makes systematic errors in its predictions. The model fails to fit the training data well and has a low accuracy on both the training and test datasets. Underfitting occurs when the model lacks the capacity to capture the complexity of the problem, typically due to having too few hidden layers or too few neurons in each layer. To resolve underfitting, one can increase the model's capacity by adding more hidden layers or neurons, or by using a more complex activation function."
      ],
      "metadata": {
        "id": "YZWxz7PYvx7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Why we use Dropout in Neural Networks?"
      ],
      "metadata": {
        "id": "yjD8OcsWvTXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropout is used in neural networks to address the issue of overfitting. Overfitting occurs when the model is too complex and fits the training data too well, resulting in poor generalization to new, unseen data. To counteract overfitting, dropout randomly drops out, or \"turns off,\" a certain number of neurons during training. This forces the model to learn multiple independent representations, making it less likely to overfit the training data. During inference, all neurons are used and the dropout rate is set to 0, so that the model can make predictions based on the complete network. Dropout is a simple and effective way to regularize deep neural networks and improve their performance on unseen data."
      ],
      "metadata": {
        "id": "zMb0ucHUv6vN"
      }
    }
  ]
}