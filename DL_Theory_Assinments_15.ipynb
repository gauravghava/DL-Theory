{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DL theory : Assingments-15**\n",
    "\n",
    "1.  Deep Learning: a. To build a DNN with five hidden layers of 100\n",
    "    neurons each, He initialization, and the ELU activation function,\n",
    "    you can use a deep learning framework such as TensorFlow or PyTorch.\n",
    "    The code for building the DNN would involve defining the\n",
    "    architecture of the network, including the number of layers and\n",
    "    neurons per layer, and the activation function. He initialization\n",
    "    can be achieved by initializing the weights with a normal\n",
    "    distribution with a mean of 0 and a standard deviation of sqrt(2/n),\n",
    "    where n is the number of input neurons. b. To train the DNN on MNIST\n",
    "    digits 0 to 4, you can use the Adam optimization algorithm and early\n",
    "    stopping to prevent overfitting. You can also save checkpoints at\n",
    "    regular intervals and save the final model for reuse later. The\n",
    "    output layer should have a softmax activation function with five\n",
    "    neurons, one for each of the digits 0 to 4. c. To tune the\n",
    "    hyperparameters, you can use techniques such as cross-validation to\n",
    "    try different combinations and see which one achieves the best\n",
    "    precision. d. To add Batch Normalization, you can insert batch\n",
    "    normalization layers between the hidden layers and the activation\n",
    "    function. Compare the learning curves to see if it converges faster\n",
    "    and produces a better model. e. To check for overfitting, you can\n",
    "    evaluate the model's performance on a validation set. If the model\n",
    "    is overfitting, you can try adding dropout to every layer to reduce\n",
    "    overfitting\n",
    "\n",
    "2.  **Transfer learning.**\n",
    "\n",
    "3.  a\\. To create a new DNN that reuses all the pretrained hidden layers\n",
    "    of the previous model, we can use the TensorFlow function\n",
    "    tf.keras.Model.layers to create a new model instance and then set\n",
    "    the trainable attribute of the hidden layers to False to freeze\n",
    "    them. The new model should have a new softmax output layer with five\n",
    "    neurons, corresponding to the five digits (0-4) in the original\n",
    "    model.\n",
    "\n",
    "4.  b\\. To train this new DNN on digits 5 to 9, we can use a small\n",
    "    subset of the MNIST dataset that only contains images of these\n",
    "    digits. We can use the Adam optimization algorithm and early\n",
    "    stopping to train the model, and measure the precision using a\n",
    "    validation set.\n",
    "\n",
    "5.  c\\. To improve the training speed, we can cache the frozen layers of\n",
    "    the model and use them as a starting point for training the new\n",
    "    output layer. This can significantly reduce the computation time\n",
    "    required to train the model.\n",
    "\n",
    "6.  d\\. By reusing fewer hidden layers and training the remaining layers\n",
    "    with a small dataset, we may be able to achieve a higher precision.\n",
    "    This is because the model is able to learn more specific features of\n",
    "    the new task with fewer layers.\n",
    "\n",
    "7.  e\\. By unfreezing the top two hidden layers and continuing to train,\n",
    "    we may be able to improve the model's performance even further. This\n",
    "    is because the model is able to learn more specific features of the\n",
    "    new task with a smaller dataset by fine-tuning the already trained\n",
    "    layers.\n",
    "\n",
    "> Name three popular activation functions. Can you draw them?\n",
    ">\n",
    "> Three popular activation functions are ReLU, sigmoid, and tanh.\n",
    ">\n",
    "> ReLU (Rectified Linear Unit) is defined as f(x) = max(0,x) and it is\n",
    "> usually used in the hidden layers of a network. It is a linear\n",
    "> function for positive inputs and zero for negative inputs.\n",
    ">\n",
    "> Sigmoid function is defined as f(x) = 1/(1+e^-x) and it is commonly\n",
    "> used in the output layer of a binary classification problems. It\n",
    "> produces an output between 0 and 1, and can be interpreted as a\n",
    "> probability.\n",
    ">\n",
    "> tanh (hyperbolic tangent) is defined as f(x) = (e^x - e^-x)/(e^x +\n",
    "> e^-x) and it is similar to sigmoid function but it produces an output\n",
    "> between -1 and 1. It is commonly used in the output layer of a binary\n",
    "> or multi-class classification problems.\n",
    ">\n",
    "> I am unable to draw the graph of these activation functions**.**"
   ],
   "id": "435c06f3-ecb0-431a-b43a-135821c0a852"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
