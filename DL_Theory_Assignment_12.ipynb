{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXNak89Eo5JGMnSV00f45L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KirtiKousik/DL_Theory_Assignments_iNeuron/blob/main/DL_Theory_Assignment_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. How does unsqueeze help us to solve certain broadcasting problems?"
      ],
      "metadata": {
        "id": "4hDrMRTOo1In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The unsqueeze method in PyTorch is used to add a new dimension to a tensor. This method can be useful when dealing with broadcasting problems. Broadcasting occurs when a smaller tensor is automatically expanded to match the shape of a larger tensor in elementwise arithmetic operations. The new dimension is added at a particular position in the tensor.\n",
        "\n",
        "- For example, consider two tensors, a and b, where a is a 1D tensor of shape (3,) and b is a 2D tensor of shape (3, 2). If we want to perform an elementwise addition operation between a and b, we need to make sure their shapes match. We can add a new dimension to a using the unsqueeze method:"
      ],
      "metadata": {
        "id": "BHFkGGBcpNiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([[4, 5], [6, 7], [8, 9]])\n",
        "a = a.unsqueeze(1)\n",
        "result = a + b\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fANvMhRSo26u",
        "outputId": "c5bea6ab-88ea-4d82-fbe4-503d37ceb46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5,  6],\n",
            "        [ 8,  9],\n",
            "        [11, 12]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this example, a has been unsqueezed at dimension 1, adding a new dimension of size 1 at position 1. This allows the elementwise addition operation to be performed between a and b without any broadcasting issues."
      ],
      "metadata": {
        "id": "vcrlwZI6pU9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. How can we use indexing to do the same operation as unsqueeze?"
      ],
      "metadata": {
        "id": "-jSZr8E-pZr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can use indexing to do the same operation as unsqueeze by adding new dimensions to a tensor using the slicing and reshaping operations.\n",
        "\n",
        "- For example, if we have a tensor a of shape (3,), we can add a new dimension to its first position using the following code:"
      ],
      "metadata": {
        "id": "CC7HZTS8p9yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = a.reshape(1, 3)"
      ],
      "metadata": {
        "id": "8-1enjEgpQvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this case, b is a tensor of shape (1, 3). The first dimension has been added using reshaping and now has size 1, whereas a was a rank-1 tensor of size 3."
      ],
      "metadata": {
        "id": "kXQBMJVDqKoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. How do we show the actual contents of the memory used for a tensor?"
      ],
      "metadata": {
        "id": "wTJisFA6Ww-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To access the contents of a Tensor in PyTorch, you can use the .detach().numpy() method. This method first detaches the Tensor from the computation history and then converts it to a NumPy array, allowing us to access the values stored in the Tensor."
      ],
      "metadata": {
        "id": "ZzTeZ2x_XLzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1, 2, 3, 4])\n",
        "print(x.detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxoyXbhjWnxI",
        "outputId": "4b4afcca-18b7-4e63-e1ca-df26b17e8a1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)"
      ],
      "metadata": {
        "id": "f224xLMoXRUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are typically added to each row of the matrix."
      ],
      "metadata": {
        "id": "Dh1BjaSCXiVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Creating a 3x3 matrix\n",
        "matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Creating a vector of size 3\n",
        "vector = torch.tensor([10, 20, 30])\n",
        "\n",
        "# Adding the vector to each row of the matrix\n",
        "result = matrix + vector\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PN6N0GnW64M",
        "outputId": "b2f32926-9b1e-4c5e-eca6-1c1d67f9bac3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[11, 22, 33],\n",
            "        [14, 25, 36],\n",
            "        [17, 28, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Do broadcasting and expand_as result in increased memory use? Why or why not?"
      ],
      "metadata": {
        "id": "ts3fauSTXykg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In PyTorch, broadcasting and expand_as operations can potentially result in increased memory usage.\n",
        "\n",
        "- When broadcasting, PyTorch creates a new tensor with the shape needed for the broadcast operation. This new tensor requires additional memory to store its values, which can lead to increased memory usage.\n",
        "\n",
        "- The expand_as operation, on the other hand, simply expands the dimensions of a tensor to match the shape of another tensor, without copying its data. This means that expand_as does not result in increased memory usage, as it simply changes the view of the tensor without creating a new one.\n",
        "\n",
        "- Here's an example to illustrate this:"
      ],
      "metadata": {
        "id": "owy5TZ4sYQ6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Creating a tensor with shape (3, 1)\n",
        "x = torch.tensor([[1], [2], [3]])\n",
        "\n",
        "# Broadcasting x to shape (3, 3)\n",
        "y = x + 1\n",
        "print(y.shape)\n",
        "# Output: torch.Size([3, 3])\n",
        "\n",
        "# Creating a tensor with shape (3, 3)\n",
        "z = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Using expand_as to match the shape of z\n",
        "w = x.expand_as(z)\n",
        "print(w.shape)\n",
        "# Output: torch.Size([3, 3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DulSpncoYHn4",
        "outputId": "d7459621-df1b-4bf8-b9e8-c9601906306a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1])\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this example, broadcasting x to shape (3, 3) creates a new tensor y that requires additional memory to store its values, while using expand_as to match the shape of z does not result in increased memory usage, as w simply changes the view of x without creating a new tensor."
      ],
      "metadata": {
        "id": "1JI37Ed8YV5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Implement matmul using Einstein summation."
      ],
      "metadata": {
        "id": "3wRD-we4X3Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Einstein summation notation is a compact representation for summations over multiple indices in arrays. In Einstein notation, we can represent matrix multiplication as a sum over indices.\n",
        "\n",
        "- Here's how we can implement the matmul operation using Einstein summation in Python:"
      ],
      "metadata": {
        "id": "tGTjlj6yYiB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def matmul(A, B):\n",
        "    return np.einsum('ij,jk->ik', A, B)\n",
        "\n",
        "# Testing the implementation\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "C = matmul(A, B)\n",
        "print(C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnYDPnkQYlwf",
        "outputId": "357f95c0-3fd3-4387-f088-e47dfba5f057"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[19 22]\n",
            " [43 50]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this implementation, the Einstein summation 'ij,jk->ik' represents the matrix multiplication of A and B, where the indices i and k represent the row and column indices of the resulting matrix C, respectively, and j represents the intermediate index over which the summation is performed. The sum is taken over all values of j, and the result is stored in C[i][k]."
      ],
      "metadata": {
        "id": "2tEBwKSnYrQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. What does a repeated index letter represent on the lefthand side of einsum?"
      ],
      "metadata": {
        "id": "Q--rkb6iX5-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In Einstein summation notation, a repeated index letter on the left-hand side of the arrow (->) represents the sum over that index.\n",
        "\n",
        "- For example, if you have an equation like np.einsum('ij,jk->ik', A, B), the repeated index j on the left-hand side represents a sum over the values of j. This is equivalent to the traditional matrix multiplication formula, where the sum is taken over the intermediate dimension.\n",
        "\n",
        "- So, in this case, for each value of i and k, the sum is taken over all values of j between 0 and the length of the intermediate dimension, resulting in the value of C[i][k].\n",
        "\n",
        "- In general, the left-hand side of the arrow in the Einstein summation notation specifies the indices and the shapes of the output array, while the indices on the right-hand side specify the shapes of the input arrays and the indices over which the sum is taken."
      ],
      "metadata": {
        "id": "x7dL9aT5Y-CY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. What are the three rules of Einstein summation notation? Why?"
      ],
      "metadata": {
        "id": "yImeCHlhYdA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The three rules of Einstein summation notation are:\n",
        "\n",
        "1. Summation over repeated indices: In Einstein summation notation, a repeated index represents a sum over the values of that index. For example, in the equation np.einsum('ij,jk->ik', A, B), the repeated index j represents a sum over all values of j between 0 and the length of the intermediate dimension.\n",
        "\n",
        "2. Specifying the shapes of the input and output arrays: The left-hand side of the arrow (->) in the Einstein summation notation specifies the indices and the shapes of the output array, while the indices on the right-hand side specify the shapes of the input arrays. For example, in the equation np.einsum('ij,jk->ik', A, B), the indices i and k on the left-hand side specify the row and column indices of the resulting matrix C, respectively, and the indices ij and jk on the right-hand side specify the shapes of the input arrays A and B.\n",
        "\n",
        "3. Broadcasting: The Einstein summation notation supports broadcasting of arrays, just like the traditional array operations in NumPy. Broadcasting allows you to perform operations on arrays with different shapes, as long as the shapes are broadcastable to a common shape.\n",
        "\n",
        "- These three rules make Einstein summation notation a compact and flexible representation for multi-dimensional array operations. By using Einstein summation notation, you can express complex array operations in a concise and readable form, which can make your code easier to understand and maintain."
      ],
      "metadata": {
        "id": "bTgj1teWZKlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. What are the forward pass and backward pass of a neural network?"
      ],
      "metadata": {
        "id": "U9l1xC3qZD9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The forward pass and backward pass are the two phases of a training iteration in a neural network.\n",
        "\n",
        "    1. Forward Pass: During the forward pass, the inputs to the network are fed through the network to produce the output predictions. This process involves matrix multiplications and activation functions, which are performed layer by layer to produce the final predictions. The forward pass calculates the predicted outputs from the network and stores intermediate values, which are needed for the backward pass.\n",
        "\n",
        "    2. Backward Pass (Backpropagation): The backward pass, also known as backpropagation, is the process of calculating the gradients of the loss function with respect to the model's parameters. The gradients are used to update the parameters in the opposite direction of the gradient to minimize the loss. The backward pass involves computing the gradients of the loss with respect to the intermediate values obtained during the forward pass. These gradients are then used to compute the gradients of the loss with respect to the model's parameters using chain rule of differentiation. The parameters are updated using an optimization algorithm, such as gradient descent, to minimize the loss.\n",
        "\n",
        "- This process of forward pass followed by backward pass is repeated several times until the loss reaches a minimum and the model has learned to make accurate predictions. The forward pass and backward pass are critical components of training a neural network and are performed for each training iteration to update the model's parameters and minimize the loss."
      ],
      "metadata": {
        "id": "GfAQ7ooOZWbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?"
      ],
      "metadata": {
        "id": "K8hv7xfJZRFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Storing the activations for intermediate layers in the forward pass is necessary for computing the gradients in the backward pass. The gradients are calculated using the chain rule of differentiation and require the intermediate activations as inputs. The activations represent the output of each layer of the network and are used as inputs to the next layer.\n",
        "\n",
        "- During the backward pass, the gradients of the loss function with respect to the intermediate activations are calculated, and these gradients are then used to compute the gradients with respect to the parameters of the network. This information is then used to update the parameters of the network in the direction that minimizes the loss.\n",
        "\n",
        "- Without storing the intermediate activations, it would not be possible to calculate the gradients in the backward pass, and the network would not be able to learn from the training data. Therefore, storing the intermediate activations is a critical step in the forward pass and enables the network to learn from the training data through backpropagation."
      ],
      "metadata": {
        "id": "SkbzF1S8ZiSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. What is the downside of having activations with a standard deviation too far away from 1?"
      ],
      "metadata": {
        "id": "9WXhxb_HZdd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Activations with a standard deviation far away from 1 can cause a number of problems in a neural network, including:\n",
        "\n",
        "    1. Exploding activations: If the standard deviation of the activations is too large, the activations may explode, meaning that the values of the activations become extremely large. This can cause numeric instability in the network and prevent the network from learning.\n",
        "\n",
        "    2. Vanishing activations: If the standard deviation of the activations is too small, the activations may vanish, meaning that the values of the activations become close to zero. This can cause the gradients to become close to zero as well, which can prevent the network from learning.\n",
        "\n",
        "    3. Slow convergence: Activations with a standard deviation far away from 1 can lead to slow convergence, as the network may take longer to learn the parameters that minimize the loss.\n",
        "\n",
        "    4. Decreased model expressiveness: Activations with a standard deviation far away from 1 can limit the expressiveness of the network, as the network may not be able to capture complex relationships between the inputs and outputs.\n",
        "\n",
        "- Therefore, it is important to keep the standard deviation of the activations close to 1 to avoid these problems and ensure that the network can learn effectively. This can be achieved by using normalization techniques, such as batch normalization, and by choosing appropriate activation functions that have output distributions with standard deviations close to 1."
      ],
      "metadata": {
        "id": "jFanuXjQZpzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. How can weight initialization help avoid this problem?"
      ],
      "metadata": {
        "id": "aQrCeaGlZsKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Weight initialization is a crucial step in training a neural network, as it determines the initial values of the model's parameters, which can have a significant impact on the network's performance. If the weights are initialized improperly, the network may not be able to learn effectively, and the activations may have a standard deviation far away from 1.\n",
        "\n",
        "- In order to avoid this problem, it is important to choose an appropriate weight initialization strategy that sets the initial values of the weights in a way that results in activations with a standard deviation close to 1. This can help ensure that the network is able to learn effectively and avoid the problems associated with activations with a standard deviation far away from 1.\n",
        "\n",
        "- One common weight initialization strategy is to initialize the weights with random values sampled from a normal distribution with mean 0 and a small standard deviation, such as 0.01. This can help ensure that the activations have a standard deviation close to 1, and that the gradients are not too small or too large, which can speed up the convergence of the network.\n",
        "\n",
        "- Another popular weight initialization strategy is the Glorot initialization, also known as the Xavier initialization, which sets the weights so that the variance of the activations is equal to the variance of the inputs. This helps ensure that the activations have a standard deviation close to 1, and that the network is able to learn effectively.\n",
        "\n",
        "- In conclusion, weight initialization plays an important role in ensuring that the network is able to learn effectively and avoid the problems associated with activations with a standard deviation far away from 1. It is important to choose an appropriate weight initialization strategy to ensure that the network converges quickly and produces accurate predictions."
      ],
      "metadata": {
        "id": "lf3eypKVZ4Df"
      }
    }
  ]
}