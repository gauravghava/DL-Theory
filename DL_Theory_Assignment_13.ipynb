{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMci+RzgWeqvUB7r/pmtWeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KirtiKousik/DL_Theory_Assignments_iNeuron/blob/main/DL_Theory_Assignment_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?"
      ],
      "metadata": {
        "id": "VQldxDlnaaOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It is generally preferable to use a Logistic Regression classifier over a classical Perceptron for several reasons:\n",
        "\n",
        "    1. Probabilistic Output: Logistic Regression models the probability of the positive class, which can provide more nuanced and interpretable predictions than a simple threshold-based decision in a Perceptron.\n",
        "\n",
        "    2. Non-Linear Decision Boundaries: Logistic Regression can model non-linear decision boundaries, as it uses the sigmoid activation function to produce probabilities, which can be more flexible than the linear decision boundaries produced by a Perceptron.\n",
        "\n",
        "    3. Robust to Outliers: Logistic Regression is more robust to outliers, as it is less sensitive to extreme values in the input data. This is because the sigmoid function is smooth and continuously varies between 0 and 1, whereas the Perceptron produces binary outputs that are sensitive to outliers.\n",
        "\n",
        "- To make a Perceptron equivalent to a Logistic Regression classifier, you can modify the Perceptron by replacing the threshold activation function with the sigmoid activation function. This transforms the Perceptron into a Logistic Regression classifier that outputs probabilities rather than binary decisions.\n",
        "\n",
        "- In essence, a Logistic Regression classifier is a Perceptron with a sigmoid activation function, which provides more nuanced and flexible predictions than a classical Perceptron."
      ],
      "metadata": {
        "id": "RlWT1GacapvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Why was the logistic activation function a key ingredient in training the first MLPs?"
      ],
      "metadata": {
        "id": "UGwMoQykai77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The logistic activation function (also known as the sigmoid activation function) was a key ingredient in training the first multi-layer perceptrons (MLPs) for several reasons:\n",
        "\n",
        "    1. Smoothness: The logistic function is smooth and continuously differentiable, which is important for training neural networks using gradient-based optimization algorithms.\n",
        "\n",
        "    2. Non-Linearity: The logistic function introduces non-linearity into the model, which allows MLPs to model complex relationships between inputs and outputs.\n",
        "\n",
        "    3. Probabilistic Interpretation: The output of the logistic function can be interpreted as a probability, which can be useful for binary classification tasks, where the goal is to predict the probability of an instance belonging to a certain class.\n",
        "\n",
        "    4 .Bounded Output: The logistic function produces outputs that are bounded between 0 and 1, which helps prevent the activations from exploding or vanishing during training, which can otherwise make it difficult for the network to learn.\n",
        "\n",
        "- In conclusion, the logistic activation function was a key ingredient in training the first MLPs because of its smoothness, non-linearity, probabilistic interpretation, and bounded output, which make it a suitable choice for binary classification tasks."
      ],
      "metadata": {
        "id": "RV8HxNsaa0NN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Name three popular activation functions. Can you draw them?"
      ],
      "metadata": {
        "id": "DjAhonSzal0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
        "- What is the shape of the input matrix X?\n",
        "- What about the shape of the hidden layer’s weight vector Wh, and the shape of its bias vector bh?\n",
        "- What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
        "- What is the shape of the network’s output matrix Y?\n",
        "- Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo and bo."
      ],
      "metadata": {
        "id": "gwqyxLEKb513"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Shape of input matrix X: (m, 10), where m is the number of instances in the training set.\n",
        "\n",
        "2. Shape of hidden layer's weight vector Wh: (10, 50), where 10 is the number of neurons in the input layer, and 50 is the number of neurons in the hidden layer.\n",
        "\n",
        "3. Shape of hidden layer's bias vector bh: (1, 50), where 50 is the number of neurons in the hidden layer.\n",
        "\n",
        "4. Shape of output layer's weight vector Wo: (50, 3), where 50 is the number of neurons in the hidden layer, and 3 is the number of neurons in the output layer.\n",
        "\n",
        "5. Shape of output layer's bias vector bo: (1, 3), where 3 is the number of neurons in the output layer.\n",
        "\n",
        "6. Shape of network's output matrix Y: (m, 3), where m is the number of instances in the training set.\n",
        "\n",
        "- The equation to compute the network's output matrix Y as a function of X, Wh, bh, Wo, and bo is given by:\n",
        "\n",
        "        Y = relu(X.dot(Wh) + bh).dot(Wo) + bo\n",
        "\n",
        "    - where .dot() represents the dot product, relu() is the ReLU activation function, and + represents element-wise addition."
      ],
      "metadata": {
        "id": "xbTJNiZycdSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function?"
      ],
      "metadata": {
        "id": "RkvyVaIDdiwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For classifying email into spam or ham, you need only one neuron in the output layer. The activation function used in the output layer for this binary classification problem is typically the sigmoid activation function. The sigmoid function maps any input value to a value between 0 and 1, which can then be interpreted as a probability of the positive class.\n",
        "\n",
        "- For tackling the MNIST problem, where the goal is to classify handwritten digits between 0 and 9, you need 10 neurons in the output layer. The activation function used in the output layer for this multi-class classification problem is typically the softmax activation function. The softmax function maps any input values to a probability distribution over the 10 classes, ensuring that the sum of all the probabilities is 1. This way, the output of the softmax can be interpreted as a set of class probabilities."
      ],
      "metadata": {
        "id": "tLr0nXU6dtfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?"
      ],
      "metadata": {
        "id": "rrSfs1godwn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Backpropagation is an algorithm used to train neural networks by computing the gradients of the cost function with respect to the network's parameters. It is used to update the network's weights in the direction of reducing the cost.\n",
        "\n",
        "- Backpropagation works by computing the gradient of the cost function with respect to each weight in the network by propagating error derivatives backwards from the output layer to the input layer. It uses the chain rule of differentiation to compute the gradients of the cost with respect to each weight in the network.\n",
        "\n",
        "- Reverse-mode autodiff, also known as backpropagation, is a method for computing derivatives of composite functions by applying the chain rule backwards. In the context of neural networks, it is used to efficiently compute the gradients of the cost function with respect to each weight in the network.\n",
        "\n",
        "- The key difference between backpropagation and reverse-mode autodiff is that backpropagation is a specific application of reverse-mode autodiff in the context of training neural networks. Reverse-mode autodiff can be used to compute gradients of any composite function, while backpropagation is specifically designed to compute gradients in neural networks."
      ],
      "metadata": {
        "id": "2sZKVhRkd62A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
      ],
      "metadata": {
        "id": "bYHUt4E3d09v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hyperparameters in an MLP (Multilayer Perceptron) that can be tuned include:\n",
        "\n",
        "    1. Number of hidden layers: Increasing the number of hidden layers can help capture more complex relationships in the data, but too many hidden layers can lead to overfitting.\n",
        "\n",
        "    2. Number of neurons in each hidden layer: Increasing the number of neurons in each hidden layer can also help capture more complex relationships, but too many neurons can lead to overfitting.\n",
        "\n",
        "    3. Activation function: The activation function can affect the model's ability to capture complex relationships in the data. Common activation functions include sigmoid, tanh, ReLU, and softmax.\n",
        "\n",
        "    4. Learning rate: The learning rate determines how quickly the model updates its weights during training. A higher learning rate can lead to faster convergence, but may result in overshooting the optimal solution. A lower learning rate can result in slower convergence, but is less likely to overshoot the optimal solution.\n",
        "\n",
        "    5. Regularization: Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function. Common regularization techniques include L1 regularization, L2 regularization, and dropout.\n",
        "\n",
        "    6. Batch size: The batch size determines the number of samples used in each iteration of the training process. A larger batch size can result in faster convergence, but can also lead to memory issues. A smaller batch size can result in slower convergence, but is less likely to lead to memory issues.\n",
        "\n",
        "- If the MLP is overfitting the training data, one can try the following hyperparameter tweaks to reduce overfitting:\n",
        "\n",
        "    1. Reduce the number of hidden layers or neurons in each hidden layer\n",
        "    2. Increase the strength of regularization (e.g., increase the strength of L2 regularization or use dropout)\n",
        "    3. Decrease the learning rate\n",
        "    4. Use a different activation function\n",
        "    5. Use early stopping to stop the training process when the model begins to overfit the training data."
      ],
      "metadata": {
        "id": "XN1x4vejeFtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard  and so on)."
      ],
      "metadata": {
        "id": "GVBDURd7d_-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess data\n",
        "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28 * 28) / 255.0\n",
        "\n",
        "# One hot encode labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Add callbacks for checkpointing and TensorBoard\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('./checkpoints/model.ckpt', save_best_only=True, save_weights_only=True)\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
        "callbacks = [checkpoint, tensorboard]\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=20, callbacks=callbacks, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcMsxZIJac-U",
        "outputId": "4d12cfa4-78b6-4494-d5d3-0647a3edfc97"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/20\n",
            "469/469 [==============================] - 10s 19ms/step - loss: 0.2195 - accuracy: 0.9349 - val_loss: 0.1106 - val_accuracy: 0.9657\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0802 - accuracy: 0.9747 - val_loss: 0.0787 - val_accuracy: 0.9748\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0509 - accuracy: 0.9840 - val_loss: 0.0650 - val_accuracy: 0.9792\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 0.0357 - accuracy: 0.9883 - val_loss: 0.0630 - val_accuracy: 0.9802\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0262 - accuracy: 0.9916 - val_loss: 0.0848 - val_accuracy: 0.9756\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.0766 - val_accuracy: 0.9798\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 0.0788 - val_accuracy: 0.9789\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0148 - accuracy: 0.9947 - val_loss: 0.0658 - val_accuracy: 0.9823\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0152 - accuracy: 0.9948 - val_loss: 0.0908 - val_accuracy: 0.9795\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0829 - val_accuracy: 0.9808\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.0995 - val_accuracy: 0.9763\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.0124 - accuracy: 0.9960 - val_loss: 0.0817 - val_accuracy: 0.9813\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.0116 - accuracy: 0.9961 - val_loss: 0.0766 - val_accuracy: 0.9827\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.0924 - val_accuracy: 0.9834\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.0939 - val_accuracy: 0.9813\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0870 - val_accuracy: 0.9828\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.1001 - val_accuracy: 0.9820\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.0964 - val_accuracy: 0.9814\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0929 - val_accuracy: 0.9831\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.1018 - val_accuracy: 0.9813\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1018 - accuracy: 0.9813\n",
            "Test Loss: 0.10175549238920212\n",
            "Test Accuracy: 0.9812999963760376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The  test accuracy is {100*test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8y5lbvmmRwX",
        "outputId": "25fa4854-8653-4fbd-d6c6-93fcdd43fb44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The  test accuracy is 98.13%\n"
          ]
        }
      ]
    }
  ]
}